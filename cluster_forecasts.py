# -*- coding: utf-8 -*-
"""Cluster Forecasts

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MGyHWposLIF2TNUFdwruTPiZ4tPh0ymW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import itertools
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv(r"/content/drive/My Drive/Colab Notebooks/Trilytics'24/sales_store_promotions.csv")

df.head()

promo_features = ['PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']

df[promo_features] = df[promo_features].fillna(0)
df[promo_features] = df[promo_features].astype(int)

df['BUSINESS_DATE'] = pd.to_datetime(df['BUSINESS_DATE'], format = '%d-%m-%Y')

df.info()

# Forecast only for stores that we have enough data on (at least 90 days)
stores = df[df['BUSINESS_DATE'] <= '2023-07-04']['STORE_KEY'].unique()
df = df[df['STORE_KEY'].isin(stores)]

# Number of stores in each cluster
df[['STORE_KEY', 'Cluster']].groupby('Cluster').nunique()

# Define the special dates
downspike_dates = [
    '2019-11-28', '2020-11-26', '2021-11-25', '2022-11-24', '2023-11-23',  # Thanksgiving
    '2019-12-25', '2020-12-25', '2021-12-25', '2022-12-25', '2023-12-25'   # Christmas
]
upspike_dates = [
    '2020-02-02', '2021-02-07', '2022-02-13', '2023-02-12'  # Super Bowl
]

# Convert to datetime
downspike_dates = pd.to_datetime(downspike_dates)
upspike_dates = pd.to_datetime(upspike_dates)

"""#<font color = red> The Get Cluster Data Function"""

from datetime import datetime

df['BUSINESS_DATE'] = pd.to_datetime(df['BUSINESS_DATE'])
df['UBER_EATS_MARKUP_DATE'] = pd.to_datetime(df['UBER_EATS_MARKUP_DATE'])
df['DOOR_DASH_MARKUP_GO_LIVE_DATE'] = pd.to_datetime(df['DOOR_DASH_MARKUP_GO_LIVE_DATE'])
df['STORE_LIVE_DATE'] = pd.to_datetime(df['STORE_LIVE_DATE'], format = "%d-%m-%Y")

def cluster_data(c_number):
  # Filter data for cluster
  cluster = df[df['Cluster'] == c_number]

  # Aggregate net sales by business date
  sales_time_series = cluster[['BUSINESS_DATE', 'NET_SALES_FINAL_USD_AMOUNT']].groupby('BUSINESS_DATE').sum()

  # Total number of stores in cluster
  total_stores_cluster = cluster['STORE_KEY'].nunique()

  # Calculate the percentage of stores on Uber Eats
  def calculate_ubereats_percentage(business_date):
      stores_on_ubereats = cluster[cluster['UBER_EATS_MARKUP_DATE'] <= business_date]['STORE_KEY'].nunique()
      return stores_on_ubereats / total_stores_cluster

  percentage_ubereats = round(sales_time_series.index.to_series().apply(calculate_ubereats_percentage)*100, 2)

  # Calculate the sum of the age of stores on Uber Eats
  cluster['ubereats_age'] = (pd.to_datetime('today') - pd.to_datetime(cluster['STORE_LIVE_DATE'])).dt.days
  cluster['ubereats_age'].fillna(0, inplace=True)
  cluster['ubereats_age'] = cluster['ubereats_age'].apply(lambda x: max(x, 0))  # Set negative values to zero  sum_ubereats_age = cluster.groupby('BUSINESS_DATE')['ubereats_age'].sum()
  sum_ubereats_age = cluster.groupby('BUSINESS_DATE')['ubereats_age'].sum()

  # Calculate the number of days on DoorDash
  cluster['doordash_age'] = (pd.to_datetime('today') - pd.to_datetime(cluster['DOOR_DASH_MARKUP_GO_LIVE_DATE'])).dt.days
  cluster['doordash_age'].fillna(0, inplace=True)
  cluster['doordash_age'] = cluster['doordash_age'].apply(lambda x: max(x, 0))  # Set negative values to zero

  # Calculate the number of days on Uber Eats
  cluster['ubereats_age_days'] = (pd.to_datetime('today') - pd.to_datetime(cluster['UBER_EATS_MARKUP_DATE'])).dt.days
  cluster['ubereats_age_days'].fillna(0, inplace=True)
  cluster['ubereats_age_days'] = cluster['ubereats_age_days'].apply(lambda x: max(x, 0))  # Set negative values to zero

  # Create a feature dataframe
  features = pd.DataFrame({
      'BUSINESS_DATE': sales_time_series.index,
      'percentage_ubereats': percentage_ubereats,
      'sum_ubereats_age': sum_ubereats_age,
      'day_of_year': sales_time_series.index.dayofyear,
      'doordash_age_days': cluster.groupby('BUSINESS_DATE')['doordash_age'].sum(),
      'ubereats_age_days': cluster.groupby('BUSINESS_DATE')['ubereats_age_days'].sum(),
      'py_net_sales': cluster.groupby('BUSINESS_DATE')['PY_NET_SALES_FINAL_USD_AMOUNT'].sum(),
      'PLATFORM_DD': cluster.groupby('BUSINESS_DATE')['PLATFORM_DD'].sum(),
      'PLATFORM_Inhouse': cluster.groupby('BUSINESS_DATE')['PLATFORM_Inhouse'].sum(),
      'PLATFORM_UE': cluster.groupby('BUSINESS_DATE')['PLATFORM_UE'].sum(),
      'FREQUENCY_Weekday': cluster.groupby('BUSINESS_DATE')['FREQUENCY_Weekday'].sum(),
      'PromotionCash': cluster.groupby('BUSINESS_DATE')['PromotionCash'].sum(),
      'PromotionKind': cluster.groupby('BUSINESS_DATE')['PromotionKind'].sum(),
      'PROMOTION_ITEM_DeliveryFee': cluster.groupby('BUSINESS_DATE')['PROMOTION_ITEM_DeliveryFee'].sum(),
      'PROMOTION_COVERAGE_Regional': cluster.groupby('BUSINESS_DATE')['PROMOTION_COVERAGE_Regional'].sum()
  }).set_index('BUSINESS_DATE')

  # Merge sales and feature data
  cluster_data = sales_time_series.merge(features, left_index = True, right_index = True)

  # Handling missing values (if any)
  cluster_data.fillna(0, inplace = True)

  # Check the prepared dataset
  return cluster_data

"""#<font color = red> Disaggregation: Simple Recent Proportions"""

def disaggregate(c_number, forecast_series, period):
  # Store the cluster level forecasts
  cluster_forecasts = pd.DataFrame(forecast_series)

  # Calculate total recent sales for each store
  recent_period = period
  recent_store_sales = df[df['Cluster'] == c_number].set_index('BUSINESS_DATE').last(recent_period).groupby('STORE_KEY')['NET_SALES_FINAL_USD_AMOUNT'].sum()

  # Calculate total recent sales for the cluster
  total_recent_sales = recent_store_sales.sum()

  # Calculate the proportion of sales for each store
  recent_store_proportions = recent_store_sales / total_recent_sales

  # Allocate forecasts to each store based on recent sales proportions
  recent_store_forecasts = pd.DataFrame(index = cluster_forecasts.index)

  for store_key, proportion in recent_store_proportions.items():
      recent_store_forecasts[store_key] = cluster_forecasts * proportion

  # recent_store_forecasts now contains the disaggregated forecasts for each store
  return recent_store_forecasts

"""#<font color = red> Disaggregation by Proportion Estimation: Random Forest"""

from sklearn.ensemble import RandomForestRegressor

def estimate_proportion_forecast(store_key):
  # Get Cluster Number
  c_no = df[df['STORE_KEY'] == store_key]['Cluster'].iloc[0]

  # Create cluster specific data
  cluster_df = df[df['Cluster'] == c_no]
  cluster_df['STORE_LIVE_DATE'] = pd.to_datetime(cluster_df['STORE_LIVE_DATE'])
  cluster_df['UBER_EATS_MARKUP_DATE'] = pd.to_datetime(cluster_df['UBER_EATS_MARKUP_DATE'])
  cluster_df['DOOR_DASH_MARKUP_GO_LIVE_DATE'] = pd.to_datetime(cluster_df['DOOR_DASH_MARKUP_GO_LIVE_DATE'])

  # Create features most likely influencing forecast proportions
  cluster_df['Store_Age'] = (pd.to_datetime(cluster_df['BUSINESS_DATE']) - pd.to_datetime(cluster_df['STORE_LIVE_DATE'])).dt.days  # Age of store as on Business Date
  cluster_df['Store_Age'].fillna(0, inplace = True)
  cluster_df['Store_Age'] = cluster_df['Store_Age'].apply(lambda x: max(x, 0))
  cluster_df['UE_Age'] = (pd.to_datetime(cluster_df['BUSINESS_DATE']) - pd.to_datetime(cluster_df['UBER_EATS_MARKUP_DATE'])).dt.days # No. of days on Uber Eats
  cluster_df['UE_Age'].fillna(0, inplace = True)
  cluster_df['UE_Age'] = cluster_df['UE_Age'].apply(lambda x: max(x, 0))  # Set negative values to zero
  cluster_df['DD_Age'] = (pd.to_datetime(cluster_df['BUSINESS_DATE']) - pd.to_datetime(cluster_df['DOOR_DASH_MARKUP_GO_LIVE_DATE'])).dt.days # No. of days on DoorDash
  cluster_df['DD_Age'].fillna(0, inplace = True)
  cluster_df['DD_Age'] = cluster_df['DD_Age'].apply(lambda x: max(x, 0))  # Set negative values to zero
  sales_by_date = cluster_df[['BUSINESS_DATE', 'NET_SALES_FINAL_USD_AMOUNT']].groupby('BUSINESS_DATE').sum()
  cluster_df = pd.merge(cluster_df, sales_by_date, how = 'left', on = 'BUSINESS_DATE')
  cluster_df['proportion'] = cluster_df['NET_SALES_FINAL_USD_AMOUNT_x'] / cluster_df['NET_SALES_FINAL_USD_AMOUNT_y']  # Proportion of cluster sales
  cluster_df['proportion'].fillna(0, inplace = True)

  # Extract data for only the particular store for which we are forecasting
  features = ['BUSINESS_DATE', 'BUSINESS_DATE_FISCAL_YEAR',
               'BUSINESS_DATE_FISCAL_DAY_OF_YEAR', 'PY_NET_SALES_FINAL_USD_AMOUNT',
               'Store_Age', 'UE_Age', 'DD_Age', 'proportion']

  cluster_df = cluster_df[cluster_df['STORE_KEY'] == store_key][features].sort_values('BUSINESS_DATE')

  # Divide the data
  forecast_horizon = 90
  X_train = cluster_df.drop(['BUSINESS_DATE', 'proportion'], axis = 1).iloc[:-forecast_horizon]
  y_train = cluster_df['proportion'].iloc[:-forecast_horizon]
  X_test = cluster_df.drop(['BUSINESS_DATE', 'proportion'], axis = 1).iloc[-forecast_horizon:]
  y_test = cluster_df['proportion'].iloc[-forecast_horizon:]

  # Standardize features
  # scaler = StandardScaler()
  # X_train[['Store_Age', 'UE_Age', 'DD_Age', 'PY_NET_SALES_FINAL_USD_AMOUNT']] = scaler.fit_transform(X_train[['Store_Age', 'UE_Age', 'DD_Age', 'PY_NET_SALES_FINAL_USD_AMOUNT']])
  # X_test[['Store_Age', 'UE_Age', 'DD_Age', 'PY_NET_SALES_FINAL_USD_AMOUNT']] = scaler.transform(X_test[['Store_Age', 'UE_Age', 'DD_Age', 'PY_NET_SALES_FINAL_USD_AMOUNT']])

  # Fit a model to predict proportion
  model = RandomForestRegressor(n_estimators = 100, max_depth = 5, random_state = 42)
  model.fit(X_train, y_train)
  model_fit = model.fit(X_train, y_train)
  forecast_proportions = model_fit.predict(X_test)

  # Create a DataFrame for the forecasts
  forecast_df = pd.DataFrame({
      'BUSINESS_DATE': X_test.index,
      'Forecast_Sales': forecast_proportions * cluster_forecasts[c_no]
  })

  # If proportion is negative, reset to 0
  forecast_df['Forecast_Sales'] = forecast_df['Forecast_Sales'].apply(lambda x: max(x, 0))

  return forecast_df

"""#<font color = blue> Cluster 0"""

cluster_0_data = cluster_data(0)
pd.concat([cluster_0_data.head(), cluster_0_data.tail()])

# Split data into train and test sets
train = cluster_0_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_0_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']

# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts = []
cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 0,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = blue> Cluster 1"""

cluster_1_data = cluster_data(1)
pd.concat([cluster_1_data.head(), cluster_1_data.tail()])

# Split data into train and test sets
train = cluster_1_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_1_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']

# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 1,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = blue> Cluster 2"""

cluster_2_data = cluster_data(2)
pd.concat([cluster_2_data.head(), cluster_2_data.tail()])

# Split data into train and test sets
train = cluster_2_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_2_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']


# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 2,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = blue> Cluster 3"""

cluster_3_data = cluster_data(3)
pd.concat([cluster_3_data.head(), cluster_3_data.tail()])

# Split data into train and test sets
train = cluster_3_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_3_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']


# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 3,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = blue> Cluster 4"""

cluster_4_data = cluster_data(4)
pd.concat([cluster_4_data.head(), cluster_4_data.tail()])

# Split data into train and test sets
train = cluster_4_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_4_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']


# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 4,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = blue> Cluster 5"""

cluster_5_data = cluster_data(5)
pd.concat([cluster_5_data.head(), cluster_5_data.tail()])

# Split data into train and test sets
train = cluster_5_data.iloc[:-90]  # Assuming the last 90 days are for testing
test = cluster_5_data.iloc[-90:]

plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'])

# Normalize all but indicator features
columns = ['percentage_ubereats', 'sum_ubereats_age',
       'day_of_year', 'doordash_age_days', 'ubereats_age_days', 'py_net_sales']
scaler = StandardScaler()
train[columns] = scaler.fit_transform(train[columns])
test[columns] = scaler.transform(test[columns])

"""##<font color = magenta> ACF-PACF Analysis"""

# Extract the sales time series
sales = train['NET_SALES_FINAL_USD_AMOUNT']

# Plot ACF
plt.figure(figsize = (10, 5))
plot_acf(sales, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF)')
plt.show()

# Plot PACF
plt.figure(figsize = (10, 5))
plot_pacf(sales, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF)')
plt.show()

# Compute the first difference
sales_diff = sales.diff().dropna()

# Plot ACF for first differenced data
plt.figure(figsize = (10, 5))
plot_acf(sales_diff, lags = 40, alpha = 0.05)
plt.title('Autocorrelation Function (ACF) - First Differenced')
plt.show()

# Plot PACF for first differenced data
plt.figure(figsize = (10, 5))
plot_pacf(sales_diff, lags = 40, alpha = 0.05, method = 'ywm')
plt.title('Partial Autocorrelation Function (PACF) - First Differenced')
plt.show()

"""##<font color = magenta> SARIMAX"""

exog_vars = ['percentage_ubereats', 'sum_ubereats_age', 'day_of_year',
             'doordash_age_days', 'ubereats_age_days', 'py_net_sales', 'PLATFORM_DD',
       'PLATFORM_Inhouse', 'PLATFORM_UE', 'FREQUENCY_Weekday', 'PromotionCash',
       'PromotionKind', 'PROMOTION_ITEM_DeliveryFee',
       'PROMOTION_COVERAGE_Regional']

# Fit the SARIMAX model
model = SARIMAX(train['NET_SALES_FINAL_USD_AMOUNT'],
                exog = train[exog_vars],
                order = (0, 1, 1), seasonal_order = (0, 1, 1, 7))
model_fit = model.fit(disp = False)

# Forecast
forecast = model_fit.get_forecast(steps = len(test),
                                  exog = test[exog_vars])

forecast_conf = forecast.conf_int()
forecast_series = forecast.predicted_mean

# Create a DataFrame for the forecasts
forecast_df = pd.DataFrame({
    'BUSINESS_DATE': test.index,
    'Predicted_Sales': forecast_series
})

# Initialize the adjusted predictions with the original forecasts
adjusted_forecast = forecast_series.copy()

# Adjust the forecasts based on special dates
for i in range(len(forecast_df)):
    date = forecast_df['BUSINESS_DATE'].iloc[i]

    if date in downspike_dates:
        adjusted_forecast[i] = 0 # Set downspikes sales to zero
    elif date in upspike_dates:
        if i > 0:
            adjusted_forecast[i] = adjusted_forecast[i-1] + forecast_series[i]  # Add the forecasted value to the previous value for upspike dates
    else:
        adjusted_forecast[i] = forecast_series[i]  # Keep the original forecast for other dates

# Create a DataFrame for the adjusted predictions
adjusted_predictions_df = pd.DataFrame({
    'BUSINESS_DATE': forecast_df['BUSINESS_DATE'],
    'Adjusted_Predicted_Sales': adjusted_forecast
})

# Plot the results
plt.figure(figsize = (12, 6))
plt.plot(train['NET_SALES_FINAL_USD_AMOUNT'], label = 'Train')
plt.plot(test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Test')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'], label='Adjusted Forecasted Sales')
#plt.plot(forecast_series, label = 'Forecast')
plt.fill_between(forecast_conf.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'k', alpha = 0.2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(test.index, test['NET_SALES_FINAL_USD_AMOUNT'], label = 'Actual Test Data', color = 'blue')
plt.plot(adjusted_predictions_df['BUSINESS_DATE'], adjusted_predictions_df['Adjusted_Predicted_Sales'],
         label='Adjusted Forecasted Sales', color = 'orange')
plt.fill_between(test.index, forecast_conf.iloc[:, 0], forecast_conf.iloc[:, 1],
                 color = 'orange', alpha = 0.2)
plt.title('Test Data and Forecast from SARIMAX Model')
plt.xlabel('Date')
plt.ylabel('Sales')
#plt.axvline(x = test[test.index == '2023-11-23'].index, color = 'red')
#plt.axvline(x = test[test.index == '2023-12-25'].index, color = 'red')
plt.legend()
plt.show()

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~test.index.isin(exclude_dates)

actual = test['NET_SALES_FINAL_USD_AMOUNT']
actual_excl = test.loc[mask, 'NET_SALES_FINAL_USD_AMOUNT']

mape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / actual)) * 100
print(f"MAPE: {mape:.2f}%")

smape = np.mean(np.abs((actual - adjusted_predictions_df['Adjusted_Predicted_Sales']) / (actual + adjusted_predictions_df['Adjusted_Predicted_Sales'])) * 200)
print(f"sMAPE: {smape:.2f}%")

mape_excl = np.mean(np.abs((actual_excl - adjusted_predictions_df['Adjusted_Predicted_Sales'][mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

cluster_forecasts.append(adjusted_predictions_df['Adjusted_Predicted_Sales'])

"""##<font color = magenta> Disaggregation: Simple Recent Proportions"""

disaggregated_forecasts = disaggregate(c_number = 5,
                                       forecast_series = adjusted_predictions_df['Adjusted_Predicted_Sales'],
                                       period = '1Y')
pd.concat([disaggregated_forecasts.head(), disaggregated_forecasts.tail()])

# Select a random store_key from the list of store keys
random_store_key = random.choice(list(disaggregated_forecasts.columns))
print(f"Randomly selected store_key: {random_store_key}")

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == random_store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecasted_sales = disaggregated_forecasts[random_store_key]

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {random_store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = green> User Interaction: Simply input the **STORE_KEY** !!"""

list_of_stores = df['STORE_KEY'].unique()
list_of_stores

# Enter the store key
store_key = 2006

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecast_dis = estimate_proportion_forecast(store_key)
forecasted_sales = forecast_dis['Forecast_Sales']

# Combine actual and forecasted sales into a single DataFrame

combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

mape = np.mean(np.abs((actual_sales - forecasted_sales) / actual_sales)) * 100
print(f"MAPE: {mape:.2f}%")


smape = np.mean(np.abs(actual_sales - forecasted_sales)*2 / (actual_sales + forecasted_sales)) * 100
print(f"sMAPE: {smape:.2f}%")

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~combined_sales.index.isin(exclude_dates)

actual = combined_sales['Actual Sales']
actual_excl = combined_sales.loc[mask, 'Actual Sales']

mape_excl = np.mean(np.abs((actual_excl - forecasted_sales[mask]) / actual_excl)) * 100
print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

# Plot the actual and forecasted sales
plt.figure(figsize = (14, 7))
plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
plt.title(f'Actual vs Forecasted Sales for Store {store_key}')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

"""#<font color = cyan> THE END"""

import ipywidgets as widgets
from IPython.display import display

store_key_widget = widgets.Dropdown(
    options=list_of_stores,
    description='Store Key:',
)

display(store_key_widget)

def on_button_clicked(b):
    store_key = store_key_widget.value
    # Your existing code here to calculate and plot the forecasts
    # Enter the store key

    # Extract actual sales for the selected store
    actual_sales = df[df['STORE_KEY'] == store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

    # Extract forecasted sales for the selected store
    forecast_dis = estimate_proportion_forecast(store_key)
    forecasted_sales = forecast_dis['Forecast_Sales']

    # Combine actual and forecasted sales into a single DataFrame

    combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
    combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

    mape = np.mean(np.abs((actual_sales - forecasted_sales) / actual_sales)) * 100
    print(f"MAPE: {mape:.2f}%")


    smape = np.mean(np.abs(actual_sales - forecasted_sales)*2 / (actual_sales + forecasted_sales)) * 100
    print(f"sMAPE: {smape:.2f}%")

    # Define dates to exclude
    exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

    # Filter out the excluded dates
    mask = ~combined_sales.index.isin(exclude_dates)

    actual = combined_sales['Actual Sales']
    actual_excl = combined_sales.loc[mask, 'Actual Sales']

    mape_excl = np.mean(np.abs((actual_excl - forecasted_sales[mask]) / actual_excl)) * 100
    print(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

    # Plot the actual and forecasted sales
    plt.figure(figsize = (14, 7))
    plt.plot(combined_sales['Actual Sales'], label = 'Actual Sales')
    plt.plot(combined_sales['Forecasted Sales'], label = 'Forecasted Sales', linestyle = '--')
    plt.title(f'Actual vs Forecasted Sales for Store {store_key}')
    plt.xlabel('Date')
    plt.ylabel('Sales')
    plt.legend()
    plt.show()

button = widgets.Button(description="Submit")
button.on_click(on_button_clicked)
display(button)

pip install streamlit

import streamlit as st


# Enter the store key
store_key = st.selectbox('Select Store Key', df['STORE_KEY'].unique())

# Extract actual sales for the selected store
actual_sales = df[df['STORE_KEY'] == store_key].set_index('BUSINESS_DATE')['NET_SALES_FINAL_USD_AMOUNT'].sort_index()[-90:]

# Extract forecasted sales for the selected store
forecast_dis = estimate_proportion_forecast(store_key)
forecasted_sales = forecast_dis['Forecast_Sales']

# Combine actual and forecasted sales into a single DataFrame
combined_sales = pd.concat([actual_sales, forecasted_sales], axis=1)
combined_sales.columns = ['Actual Sales', 'Forecasted Sales']

mape = np.mean(np.abs((actual_sales - forecasted_sales) / actual_sales)) * 100
smape = np.mean(np.abs(actual_sales - forecasted_sales) * 2 / (actual_sales + forecasted_sales)) * 100

st.write(f"MAPE: {mape:.2f}%")
st.write(f"sMAPE: {smape:.2f}%")

# Define dates to exclude
exclude_dates = pd.to_datetime(['2023-11-23', '2023-12-25'])

# Filter out the excluded dates
mask = ~combined_sales.index.isin(exclude_dates)

actual_excl = combined_sales.loc[mask, 'Actual Sales']
mape_excl = np.mean(np.abs((actual_excl - forecasted_sales[mask]) / actual_excl)) * 100
st.write(f"MAPE excluding the 2 spikes: {mape_excl:.2f}%")

# Plot the actual and forecasted sales
fig, ax = plt.subplots(figsize=(14, 7))
ax.plot(combined_sales['Actual Sales'], label='Actual Sales')
ax.plot(combined_sales['Forecasted Sales'], label='Forecasted Sales', linestyle='--')
ax.set_title(f'Actual vs Forecasted Sales for Store {store_key}')
ax.set_xlabel('Date')
ax.set_ylabel('Sales')
ax.legend()
st.pyplot(fig)

!jupyter nbconvert --to script Cluster_Forecasts.ipynb

with open('requirements.txt', 'w') as f:
    f.write('streamlit\n')
    f.write('pandas\n')
    f.write('numpy\n')
    f.write('matplotlib\n')
    f.write('scikit-learn\n')
    f.write('joblib\n')
